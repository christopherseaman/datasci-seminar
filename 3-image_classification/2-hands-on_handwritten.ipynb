{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Install/import libraries\n",
    "As usual, remember to use a virtual environment!\n",
    "\n",
    "### Download data\n",
    "You can download the EMNIST dataset from the official website. Make sure to provide clear instructions on how to download and extract the data.\n",
    "\n",
    "### Define helper functions \n",
    "It's a good idea to preprocess the data to make it easier to work with. You can create subsets of the data for training, validation, and testing. Also, since the labels in the original dataset are encoded as integers, it may be helpful to create a dictionary that maps the integer labels to their corresponding characters.\n",
    "\n",
    "## Example: Classify 0 vs 1\n",
    "\n",
    "### Pre-built models classifying 0/1\n",
    " - Logistic regression\n",
    " - RandomForest\n",
    " - XGBoost\n",
    " - Neural network\n",
    "\n",
    "### Evaluate/compare model performance\n",
    " - Confusion matrix: A table that shows the number of true positives, true negatives, false positives, and false negatives for a binary classification problem.\n",
    " - Accuracy: The proportion of correct predictions over the total number of predictions.\n",
    " - Precision: The proportion of true positives over the total number of positive predictions.\n",
    " - Recall: The proportion of true positives over the total number of actual positives.\n",
    " - F1 score: The harmonic mean of precision and recall, which balances both metrics and gives equal weight to both.\n",
    "\n",
    "## Self-guided: Classify digits\n",
    "DIY baseline model (your choice - logistic regression? decision tree? CNN?) as a starting point. Try out different options, hyperparameters, and feature representations to improve the performance of their classifier. Any opportunity for `feature engineering`?\n",
    "\n",
    "## Self-guided: Classify all symbols\n",
    "### Evaluate the model\n",
    "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
    "\n",
    "### Investigate subsets\n",
    "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
    "\n",
    "### Improve performance\n",
    "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (once per virtual environment)\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emnist\n",
    "\n",
    "# ML packages\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# XGBoost (SVM)\n",
    "from xgboost import XGBClassifier\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Constants\n",
    "SIZE = 28\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # or 3 to suppress all warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def int_to_char(label):\n",
    "    \"\"\"Convert an integer label to the corresponding uppercase character.\"\"\"\n",
    "    if label < 10:\n",
    "        return str(label)\n",
    "    elif label < 36:\n",
    "        return chr(label - 10 + ord('A'))\n",
    "    else:\n",
    "        return chr(label - 36 + ord('a'))\n",
    "\n",
    "def show_image(row):\n",
    "    \"\"\"Display a single image and its corresponding label.\"\"\"\n",
    "    image = row['image']\n",
    "    label = row['label']\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Label: ' + int_to_char(label))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_grid(data, title=None, num_cols=5, figsize=(20, 10)):\n",
    "    \"\"\"\n",
    "    Display a list of images as a grid of num_cols columns.\n",
    "    images: a list of images, each represented as a 28x28 numpy array\n",
    "    labels: a list of labels, one for each image\n",
    "    title: (optional) a title for the plot\n",
    "    num_cols: (optional) number of columns to use in the grid\n",
    "    figsize: (optional) size of the figure\n",
    "    \"\"\"\n",
    "    num_images = len(data)\n",
    "    num_rows = (num_images - 1) // num_cols + 1\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            index = i * num_cols + j\n",
    "            if index < num_images:\n",
    "                axes[i, j].imshow(data.iloc[index]['image'], cmap='gray')\n",
    "                axes[i, j].axis('off')\n",
    "                label = int_to_char(data.iloc[index]['label'])\n",
    "                axes[i, j].set_title(label)\n",
    "    plt.show()\n",
    "\n",
    "# Get a random image of a given label from the dataset\n",
    "def get_image_by_label(data, label):\n",
    "    \"\"\"Get a random image of a given label from the dataset.\"\"\"\n",
    "    images = data[data['label'] == label]['image'].tolist()\n",
    "    return random.choice(images)\n",
    "\n",
    "# Plot the training and validation accuracy during the training of a model\n",
    "def plot_accuracy(history):\n",
    "    \"\"\"Plot the training and validation accuracy during the training of a model.\"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training and validation loss during the training of a model\n",
    "def plot_loss(history):\n",
    "    \"\"\"Plot the training and validation loss during the training of a model.\"\"\"\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Normalize the pixel values of the images in the dataset to have zero mean and unit variance\n",
    "def normalize_images(images):\n",
    "    \"\"\"Normalize the pixel values of the images in the dataset to have zero mean and unit variance.\"\"\"\n",
    "    images = np.array(images)\n",
    "    mean = images.mean()\n",
    "    std = images.std()\n",
    "    images = (images - mean) / std\n",
    "    return images.tolist()\n",
    "\n",
    "# Augment images by applying random rotations, translations, and zooms to the original images in the dataset\n",
    "# Example: randomly select 5 images from the training set and \"augment\" them by applying random rotations, translations, and zooms\n",
    "# aug_images, aug_labels = augment_images(train['image'], train['label'], num_aug=5, rot_range=15, trans_range=5, zoom_range=[0.9, 1.1])\n",
    "def augment_images(images, labels, batch_size):\n",
    "    \"\"\"Generate augmented images by applying random rotations, translations, and zooms to the original images in the dataset.\"\"\"\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "    )\n",
    "    generator = datagen.flow(\n",
    "        images,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Extract the training split as images and labels\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "\n",
    "# Add columns for each pixel value (28x28 = 784 columns)\n",
    "train = pd.DataFrame()\n",
    "\n",
    "# Add a column with the image data as a 28x28 array\n",
    "train['image'] = list(image)\n",
    "train['image_flat'] = train['image'].apply(lambda x: np.array(x).reshape(-1))\n",
    "\n",
    "# Add a column showing the label\n",
    "train['label'] = label\n",
    "\n",
    "# Convert labels to characters\n",
    "class_label = np.array([int_to_char(l) for l in label])\n",
    "\n",
    "# Add a column with the character corresponding to the label\n",
    "train['class'] = class_label\n",
    "\n",
    "# Repeat for the test split\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "class_label = np.array([int_to_char(l) for l in label])\n",
    "valid = pd.DataFrame()\n",
    "valid['image'] = list(image)\n",
    "valid['image_flat'] = valid['image'].apply(lambda x: np.array(x).reshape(-1))\n",
    "valid['label'] = label\n",
    "valid['class'] = class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the helper functions\n",
    "\n",
    "# Plot a random image from the training set\n",
    "index = np.random.randint(0, len(train))\n",
    "show_image(train.iloc[index])\n",
    "\n",
    "# Show a set of 25 images in a 5x5 grid\n",
    "show_grid(train[:25], title='First 25 images')\n",
    "\n",
    "# Isn't it nice to have a helper function to do this for you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 vs 1 Classifier: Subset the data\n",
    "\n",
    "# Subset `train` and `valid` to only include 0s and 1s\n",
    "symbols_list = ['0', '1']\n",
    "\n",
    "mask_train = train['class'].apply(lambda x: x in symbols_list)\n",
    "train_01 = train[mask_train]\n",
    "train_01.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mask_valid = valid['class'].apply(lambda x: x in symbols_list)\n",
    "valid_01 = valid[mask_valid]\n",
    "valid_01.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary for performance metrics\n",
    "metrics_dict = {}\n",
    "\n",
    "# show_grid(train_01[:25], title=\"First 25 images of 0's and 1's\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 vs 1 Classifier: (optional) We can define all the metrics we want to track in a dictionary\n",
    "metrics = {\n",
    "    'task': '0_vs_1',\n",
    "    'logistic_regression': {\n",
    "        'confusion_matrix': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'confusion_matrix': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'confusion_matrix': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    },\n",
    "    'neural_network': {\n",
    "        'confusion_matrix': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9992566897918731,\n",
       " 'precision': 0.9987375729840618,\n",
       " 'recall': 0.9998420221169037,\n",
       " 'f1': 0.9992894923817794,\n",
       " 'confusion_matrix': array([[5770,    8],\n",
       "        [   1, 6329]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0 vs 1 Classifier: RandomForest\n",
    "task = '0_vs_1'\n",
    "model_name = 'random_forest'\n",
    "\n",
    "# Initialize random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate model\n",
    "rf_clf.fit(train_01['image_flat'].tolist(), train_01['label'])\n",
    "y_pred = rf_clf.predict(valid_01['image_flat'].tolist())\n",
    "\n",
    "# Calculate performance metrics\n",
    "acc = accuracy_score(valid_01['label'], y_pred)\n",
    "prec = precision_score(valid_01['label'], y_pred)\n",
    "rec = recall_score(valid_01['label'], y_pred)\n",
    "f1 = f1_score(valid_01['label'], y_pred)\n",
    "cm = confusion_matrix(valid_01['label'], y_pred)\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "metrics_dict[task] = {}\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  'confusion_matrix': cm}\n",
    "\n",
    "display(metrics_dict[task][model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9972745292368682,\n",
       " 'precision': 0.9965305156915313,\n",
       " 'recall': 0.9982622432859399,\n",
       " 'f1': 0.9973956278115382,\n",
       " 'confusion_matrix': array([[5756,   22],\n",
       "        [  11, 6319]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0 vs 1 Classifier: Logistic Regression \n",
    "task = '0_vs_1'\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "# When running without scaling the data, the model does not converge\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_01['image_flat'].tolist())\n",
    "valid_scaled = scaler.transform(valid_01['image_flat'].tolist())\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train and evaluate model\n",
    "lr_clf.fit(train_scaled, train_01['label'])\n",
    "y_pred = lr_clf.predict(valid_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "acc = accuracy_score(valid_01['label'], y_pred)\n",
    "prec = precision_score(valid_01['label'], y_pred)\n",
    "rec = recall_score(valid_01['label'], y_pred)\n",
    "f1 = f1_score(valid_01['label'], y_pred)\n",
    "cm = confusion_matrix(valid_01['label'], y_pred)\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "metrics_dict[task] = {}\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  'confusion_matrix': cm}\n",
    "\n",
    "display(metrics_dict[task][model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9994218698381235,\n",
       " 'precision': 0.999210484762356,\n",
       " 'recall': 0.9996840442338073,\n",
       " 'f1': 0.9994472084024323,\n",
       " 'confusion_matrix': array([[5773,    5],\n",
       "        [   2, 6328]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0 vs 1 Classifier: XGBoost\n",
    "task = '0_vs_1'\n",
    "model_name = 'xgboost'\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate model\n",
    "xgb_clf.fit(train_01['image_flat'].tolist(), train_01['label'])\n",
    "y_pred = xgb_clf.predict(valid_01['image_flat'].tolist())\n",
    "\n",
    "# Calculate performance metrics\n",
    "acc = accuracy_score(valid_01['label'], y_pred)\n",
    "prec = precision_score(valid_01['label'], y_pred)\n",
    "rec = recall_score(valid_01['label'], y_pred)\n",
    "f1 = f1_score(valid_01['label'], y_pred)\n",
    "cm = confusion_matrix(valid_01['label'], y_pred)\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "metrics_dict[task] = {}\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  'confusion_matrix': cm}\n",
    "\n",
    "display(metrics_dict[task][model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2280/2280 [==============================] - 5s 2ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.7836 - val_accuracy: 0.5228\n",
      "Epoch 2/10\n",
      "2280/2280 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.8640 - val_accuracy: 0.5228\n",
      "Epoch 3/10\n",
      "2280/2280 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.9333 - val_accuracy: 0.5228\n",
      "Epoch 4/10\n",
      "2280/2280 [==============================] - 6s 3ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.9119 - val_accuracy: 0.5228\n",
      "Epoch 5/10\n",
      "2280/2280 [==============================] - 6s 3ms/step - loss: 7.1154e-04 - accuracy: 0.9998 - val_loss: 0.9281 - val_accuracy: 0.5228\n",
      "Epoch 6/10\n",
      "2280/2280 [==============================] - 6s 3ms/step - loss: 5.2591e-04 - accuracy: 0.9998 - val_loss: 0.9310 - val_accuracy: 0.5228\n",
      "Epoch 7/10\n",
      "2280/2280 [==============================] - 5s 2ms/step - loss: 3.3807e-04 - accuracy: 0.9999 - val_loss: 0.9890 - val_accuracy: 0.5228\n",
      "Epoch 8/10\n",
      "2280/2280 [==============================] - 6s 3ms/step - loss: 5.4106e-04 - accuracy: 0.9998 - val_loss: 1.0282 - val_accuracy: 0.5228\n",
      "Epoch 9/10\n",
      "2280/2280 [==============================] - 5s 2ms/step - loss: 2.3925e-04 - accuracy: 0.9999 - val_loss: 1.0796 - val_accuracy: 0.5228\n",
      "Epoch 10/10\n",
      "2280/2280 [==============================] - 6s 3ms/step - loss: 3.3806e-04 - accuracy: 0.9999 - val_loss: 1.1030 - val_accuracy: 0.5228\n",
      "379/379 [==============================] - 1s 2ms/step - loss: 1.1030 - accuracy: 0.5228\n",
      "379/379 [==============================] - 0s 791us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5227948427200317,\n",
       " 'precision': 0.522794846382557,\n",
       " 'recall': 1.0,\n",
       " 'f1': 0.686625447445493,\n",
       " 'confusion_matrix': array([[   0, 5778],\n",
       "        [   0, 6330]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0 vs 1 Classifier: Neural Network\n",
    "task = '0_vs_1'\n",
    "model_name = 'neural_network'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Convert data to tensor\n",
    "train_images = np.array(train_01['image'])\n",
    "train_images = np.array(list(map(lambda x: np.reshape(x, (28, 28, 1)), train_images)))\n",
    "train_images = train_images / 255.0\n",
    "train_labels = np.array(train_01['label'])\n",
    "valid_images = np.array(valid_01['image'])\n",
    "valid_images = np.array(list(map(lambda x: np.reshape(x, (28, 28, 1)), test_images)))\n",
    "valid_images = test_images / 255.0\n",
    "valid_labels = np.array(valid_01['label'])\n",
    "\n",
    "# Initialize neural network model\n",
    "model = Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(valid_images, valid_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(valid_images, valid_labels)\n",
    "y_pred = (model.predict(valid_images) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate performance metrics\n",
    "prec = precision_score(valid_labels, y_pred)\n",
    "rec = recall_score(valid_labels, y_pred)\n",
    "f1 = f1_score(valid_labels, y_pred)\n",
    "cm = confusion_matrix(valid_labels, y_pred)\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "metrics_dict[task] = {}\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  'confusion_matrix': cm}\n",
    "\n",
    "# Display performance metrics\n",
    "display(metrics_dict[task][model_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/storage/datasci-seminar/3-image_classification/2-hands-on_handwritten.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bhopeful-tyrannulet/storage/datasci-seminar/3-image_classification/2-hands-on_handwritten.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmelt(df, id_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m], var_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, value_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bhopeful-tyrannulet/storage/datasci-seminar/3-image_classification/2-hands-on_handwritten.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Split the metrics column into individual columns\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bhopeful-tyrannulet/storage/datasci-seminar/3-image_classification/2-hands-on_handwritten.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m df[[\u001b[39m'\u001b[39;49m\u001b[39mmetric\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m]] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df[\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bhopeful-tyrannulet/storage/datasci-seminar/3-image_classification/2-hands-on_handwritten.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Create bar plot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bhopeful-tyrannulet/storage/datasci-seminar/3-image_classification/2-hands-on_handwritten.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m sns\u001b[39m.\u001b[39mbarplot(x\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, hue\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m, data\u001b[39m=\u001b[39mdf, ci\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py:3966\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_frame(key, value)\n\u001b[1;32m   3965\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, (Series, np\u001b[39m.\u001b[39mndarray, \u001b[39mlist\u001b[39m, Index)):\n\u001b[0;32m-> 3966\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_array(key, value)\n\u001b[1;32m   3967\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   3968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py:4008\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4003\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4004\u001b[0m     \u001b[39m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[1;32m   4005\u001b[0m     \u001b[39m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[1;32m   4007\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m-> 4008\u001b[0m         check_key_length(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns, key, value)\n\u001b[1;32m   4009\u001b[0m         \u001b[39mfor\u001b[39;00m k1, k2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(key, value\u001b[39m.\u001b[39mcolumns):\n\u001b[1;32m   4010\u001b[0m             \u001b[39mself\u001b[39m[k1] \u001b[39m=\u001b[39m value[k2]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexers/utils.py:402\u001b[0m, in \u001b[0;36mcheck_key_length\u001b[0;34m(columns, key, value)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m columns\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m    401\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(value\u001b[39m.\u001b[39mcolumns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(key):\n\u001b[0;32m--> 402\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mColumns must be same length as key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[39m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(columns\u001b[39m.\u001b[39mget_indexer_non_unique(key)[\u001b[39m0\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(value\u001b[39m.\u001b[39mcolumns):\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "# 0 vs 1 Classifier: Compare models\n",
    "\n",
    "# Convert metrics_dict to DataFrame\n",
    "df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Transpose the DataFrame to make task names as rows and model names as columns\n",
    "df = df.T.reset_index()\n",
    "\n",
    "# Melt the DataFrame to convert the columns into a 'variable' column and a 'value' column\n",
    "df = pd.melt(df, id_vars=['index'], var_name='model', value_name='metrics')\n",
    "\n",
    "# Split the metrics column into individual columns\n",
    "df[['metric', 'value']] = pd.DataFrame(df['metrics'].tolist())\n",
    "\n",
    "# Create bar plot\n",
    "sns.barplot(x='index', y='value', hue='metric', data=df, ci=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
